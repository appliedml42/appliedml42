<p align="center">
<img src="https://user-images.githubusercontent.com/88844341/205467453-34cb19b2-47ad-4d44-a3d7-80a67d073ea4.jpg">
</p>

I have been using Machine Learning to bring value to organizations like Amazon and Tinder since 2011. Andrew Ng's lecture at Amazon introduced me to Deep Learning in the early 2010s. I have been hooked ever since. 
<p align="center">
  <b>Please reach out if you would like to consult on your AI/ML project.</b>
  <br><br>
  <a href="https://www.linkedin.com/in/abhishekpatnia/">
    <img src="https://img.shields.io/badge/linkedin-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white" />
  </a>&nbsp;&nbsp;
  <a href="https://twitter.com/appliedml42">
    <img src="https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white" />
  </a>&nbsp;&nbsp;
  <a href="mailto:appliedml42@gmail.com">
    <img src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white" />
  </a>
</p>

## Fresh ðŸ”¥
<!-- TWITTER:START -->
- [@_akhaliq: .@diffuserslib now supports Stable Diffusion 2try is out in a few lines of codepip install diffusers](https://twitter.com/_akhaliq/status/1596198926268485634)
- [@appliedml42: Just implemented vector search using @Spotify Annoy library and @ThePSF multiprocessing. It&#39;s amazing that using multiprocessing you can implement a very usable data explorer. Usable: 4M vectors, 1280 dimensions, on 64 cores returns results in under 3 seconds.](https://twitter.com/appliedml42/status/1594018538926186497)
- [@jaschasd: If there is one thing the deep learning revolution has taught us, it&#39;s that neural nets will outperform hand-designed heuristics, given enough compute and data. But we still use hand-designed heuristics to train our models. Let&#39;s replace our optimizers with trained neural nets!](https://twitter.com/jaschasd/status/1593466553642627079)
- [@cwolferesearch: What&#39;s the best learning rate schedule to use for training a neural net? This is a simple question that pretty much any deep learning practitioner will ask. I argue that cyclical LR schedules are most practical. Here&#39;s why... ðŸ§µ [1/6]](https://twitter.com/cwolferesearch/status/1590466829625225217)
- [@ericjang11: Great overview blog post on why transformers need certain optimization tricks that aren&#39;t needed by other architectures](https://twitter.com/ericjang11/status/1589542707449704448)
<!-- TWITTER:END -->

## Learning this year ðŸ“š
<details>
  <summary>Billion Scale Language Models</summary>
</details>
<details>
  <summary>Stable Diffusion</summary>
</details>

## Resume ðŸ’¼
<details>
  <summary>2019 - </summary>
</details>
<details>
  <summary>2015 - 2019</summary>
</details>
<details>
  <summary>2011 - 2015</summary>
</details>

